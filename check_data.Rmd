---
title: "DM1"
author: "Cong Bang Huynh"
date: "12/11/2020"
output: word_document
---


```{r}
#devtools::install_github("clepadellec/ClustersAnalysis")
```

```{r}
library(ClustersAnalysis)
```





```{r}
library(openxlsx)
library(ClustersAnalysis)
```

```{r}
df=read.csv('C:/Users/DELL/Desktop/Master SISE/Github/DataMining/Train_data.csv')
```



```{r}
head(df,10)
```



```{r}
df_not_class=df[,-42]
head(df_not_class)
```

###### variable qualitative #########


```{r}
var_quanti=sapply(df_not_class, function(x) is.factor(x)| is.character(x)|length(unique(x))<50)

```

```{r}
var=var_quanti==FALSE
```



```{r}
df_not_class_quanti=df_not_class[,var]
```


```{r}
head(df_not_class_quanti,10)
```

```{r}
data=cbind(df_not_class_quanti,df$class)
```


```{r}
head(data,10)
```

######################## Test ###########################################################


```{r}
object=multivariate_object(data,22)
```

```{r}
m_test.value(object = object,i=1)
```

#les variables hot/dst_bytes/src_bytes/srv_count caracterise la classe 'normal' le plus mauvais


```{r}
m_test.value(object = object,i=2)
```

# les variables hot/dst_bytes/src_bytes/srv_count caracterise la classe 'normal' le plus mauvais:

```{r}
data_final=data[,-c(2,3,5)]
```

```{r}
object=multivariate_object(data_final,19)
```



# Rapport de correlation

```{r}
### R^2

m_R2_multivariate(object, rescale = TRUE)


```





```{r}
colnames(data_final)
```


# fisher test


```{r}
u_object=Univariate_object(data_final,19)
```



```{r}
u_fisher_test_all(u_object)
```






# Le nombre des valeurs differentes pour chaque colonne

```{r}
for (i in 1:18){
  a=length(unique(data_final[,i]))
  print(paste(colnames(data_final[i]), a))
}
```



#################### TEST AVEC ARBRE DE DECISION ################################################
#TEST AVEC ARBRE DE DECISION

##SÃ©parer en train-test (0.75-0.25)

```{r}
colnames(data_final)[19]='class'
```


```{r}
n=nrow(data_final)
ind_test=sample(1:n, n*0.25, replace = FALSE, prob = NULL)
data_train=data_final[-ind_test,]
data_test=data_final[ind_test,]
```



```{r}
head(data_train)
```




##Lancer Arbre de decision

```{r}
library(rpart)
data_train_arbre=rpart(class~.,data=data_train, method = "class")
plot(data_train_arbre)
text(data_train_arbre)

```

```{r}
library(rpart.plot)

rpart.plot(data_train_arbre)
```

## Predire avec arbre de decision



```{r}
predict=predict(data_train_arbre, data_test,"class")
```


## Calculer les metriques

```{r}
# Matrice de confusion

conf1=table(data_test$class,predict)
print(conf1)

# Taux d'erreur

print(1-sum(diag(conf1))/sum(conf1))

# Taux rappel

print(conf1[2,2]/sum(conf1["normal",]))

# Taux precision

print(conf1[2,2]/sum(conf1[,"normal"]))
```


## tuning les hyperparametres

```{r}
para=rpart.control(minsplit =10, minbucket = 2)
data_train_arbre2=rpart(class~., data_train, method = "class", control = para)

#print(DFApp_arb2)

rpart.plot(data_train_arbre2)

predict2=predict(data_train_arbre2, data_test, type="class")
```


```{r}
# Matrice de confusion

conf2=table(data_test$class,predict2)
print(conf2)

# Taux d'erreur

print(1-sum(diag(conf2))/sum(conf2))

# Taux rappel

print(conf2[2,2]/sum(conf2["normal",]))

# Taux precision

print(conf2[2,2]/sum(conf2[,"normal"]))

```









































