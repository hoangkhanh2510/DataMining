---
title: "DM1"
author: "Cong Bang Huynh"
date: "12/11/2020"
output: word_document
---


```{r}
install.packages("devtools")
library(devtools)
devtools::install_github("clepadellec/ClustersAnalysis")
install.packages("readr")
```


```{r}
library(e1071)
library(Metrics)
library(caret)
library(openxlsx)
library(ClustersAnalysis)
library(rpart) # pour arbre de decision
library(rpart.plot) # Pour la représentation de l’arbre de décision
library(ROCR)  # tracer la courbe de Roc
library(Metrics) # calculer aire sous la courbe de Roc
library(gbm) # gradient boosting
#library(tidyverse)


```


```{r}
#setwd("/Users/hoangkhanhle/Desktop/School/Master 2/Data Mining/Projet/")
df=read.csv("Train_data.csv")
#df=read.csv("C:/Users/DELL/Desktop/Master SISE/Github/Ahpine/DataMining/Train_data.csv")
```

#Analyse globale 

```{r}
#Les 10 premières observations
head(df,10)
```


```{r}
#Information sur le jeu de données
print(str(df))
```
On a dans ce jeu de données 25192 observations et 42 variables dont 4 qui sont qualitative. La dernière variable "class" est la variable que nous cherchons à prédire.

```{r}
#Répartition de la variable class
prop.table(table(df$class))
```

Dans ce jeu de données nous avons 47% de connexion anormal et 53% de connexion normal 

```{r}
#Résumé des données
summary(df)
```
On voit que nous avons des ordres de grandeurs différents sur certaines variables nottament les variables duration, src_bytes et dst_bytes qui ont des valeurs en moyenne beaucoup plus élevés que les autres variables. 

#Constitution de notre jeu de données X
```{r}
#Jeu de données sans la variable predictive
df_not_class=df[,-42]
head(df_not_class)
```

###### variable qualitative #########


```{r}
var_quanti=sapply(df_not_class, function(x) is.factor(x)| is.character(x)|length(unique(x))<50)

```


```{r}
var=var_quanti==FALSE
```


```{r}
#Jeu de données avec seulement les variables quantitative
df_not_class_quanti=df_not_class[,var]
```


```{r}
#10 première obs
head(df_not_class_quanti,10)
```

```{r}
#Donnée final avec les variables quanti et la variable class
data=cbind(df_not_class_quanti,df$class)
```


```{r}
head(data,10)
```

# Test et selection de variables


```{r}
object=multivariate_object(data,22)
```

```{r}
m_test.value(object = object,i=1)
```

Les variables hot/dst_bytes/src_bytes/srv_count caracterise la classe 'normal' le plus mauvais


```{r}
x=m_test.value(object = object,i=2)
x$pvalue<=0.05
```

Les variables hot/dst_bytes/src_bytes/srv_count caracterise la classe 'normal' le plus mauvais:

```{r}
#Nous gardons seulement les variables significatifs
data_final=data[,-c(2,3,5)]
```

```{r}
object=multivariate_object(data_final,19)
```


# Rapport de correlation

```{r}
### R^2
m_R2_multivariate(object, rescale = TRUE)
```

```{r}
#Nom des variables de notre dataframe final
colnames(data_final)
```

# fisher test

```{r}
u_object=Univariate_object(data_final,19)
```

```{r}
u_fisher_test_all(u_object)
```


# Le nombre des valeurs differentes pour chaque colonne

```{r}
for (i in 1:18){
  a=length(unique(data_final[,i]))
  print(paste(colnames(data_final[i]), a))
}
```

##Séparer en train-test (0.75-0.25)

```{r}
# renommer le nom de la colonne cible dans le data_final
colnames(data_final)[19]='class'
```


```{r}
# nombre des lignes de data_final
n=nrow(data_final)
# les indices des observations test
ind_test=sample(1:n, n*0.25, replace = FALSE, prob = NULL)
# l'ensemble d'apprentissage
data_train=data_final[-ind_test,]
#l'ensemble test
data_test=data_final[ind_test,]
```


###################ACP#######################################################################################


```{r}
#ACP + clustering sur l'apprentissage (data_train)
m_data_train=multivariate_object(data_train,19)
m_kmean_clustering_plot(m_data_train, interact = FALSE, rescale = TRUE)
```



```{r}
#ACP + clustering sur le test (data_test)
m_data_test=multivariate_object(data_test,19)
m_kmean_clustering_plot(m_data_train, interact = FALSE)
```



```{r}
#ACP + clustering sur le test (data_test)
m_data_total=multivariate_object(data_final,19)
m_kmean_clustering_plot(m_data_total, interact = FALSE, i=1,j=2)

```



#################################################################################################################
########################################### ARBRE DE DECISION ###################################################
#################################################################################################################





```{r}
#Lancer automatique Arbre de decision sur l'ensemble d'apprentissage
data_train_arbre=rpart(class~.,data=data_train, method = "class")
#plot 
rpart.plot(data_train_arbre)

```


```{r}
# prédiction sur l'ensemble test
predict_arb=predict(data_train_arbre, data_test,"class")
```


```{r}
# Matrice de confusion
pdf('1.pdf')

conf1=table(data_test$class,predict_arb)
print(conf1)

# Taux d'erreur

print(1-sum(diag(conf1))/sum(conf1))

# Taux rappel

print(conf1[2,2]/sum(conf1["normal",]))

# Taux precision

print(conf1[2,2]/sum(conf1[,"normal"]))
```


## tuning les hyperparametres


```{r}
#creation une liste des hyperparametres
gs <- list(minsplit = c(10, 20, 30, 50, 100),
           maxdepth = c(5,7,8,9,10, 15,20)) 
#transformer gs en data frame grid
gs=expand.grid(gs)
gs
```




```{r}
#nombre de ligne de gs
n=nrow(gs)
#creation un vecteur de longueur n
err=rep(0,n)
#creation un vecteur de longueur n
rappel=rep(0,n)
#creation un vecteur de longueur n
precision=rep(0,n)

# recherche de grille
for (i in 1:n){
  para=rpart.control(minsplit =gs$minsplit[i], maxdepth = gs$maxdepth[i])
  arb=rpart(class~., data_train, method = "class", control = para, minbucket = 2)
  predict_arb=predict(arb, data_test, type='class')
  conf=table(data_test$class,predict_arb)
  e=1-sum(diag(conf))/sum(conf)
  rap=conf[2,2]/sum(conf["normal",])
  pre=conf[2,2]/sum(conf[,"normal"])
  err[i]=e
  rappel[i]=rap
  precision[i]=pre
}

#dataframe qui contient des erreurs, taux rappel et precision
gs=cbind(cbind(cbind(gs,err),rappel), precision)
print(gs)  
```







# SVM
#################################################################################################################
################################################### SVM #########################################################
#################################################################################################################



# Validation croise sur l'ensemble de l'apprentissage et tester sur le test


```{r}
data_svm=data_train[,]
data_svm$class=as.factor(data_train$class)
```




```{r}
svm_fit = svm (class ~. , data =data_svm, type ="C-classification", kernel ="radial" , cross =5)
```


```{r}
predict_svm=predict(svm_fit, data_test, type='raw')
```



```{r}
#matrice de confusion en utilisant caret
confusionMatrix(data=predict_svm,as.factor(data_test$class), positive = 'normal')
```


# Optimisation des hyperparametres

```{r}
#c_seq = c (1 ,10 ,50)
#eps_seq = c (0.05 ,0.1 ,0.5)
#kernel = c("linear", 'polynomial','radial') 
#svm_grid_search = tune ( method = svm , class ~. ,
#data =data_svm , ranges = list ( epsilon = eps_seq , kernel=kernel, cost = c_seq ) )
#print (svm_grid_search)
#plot (svm_grid_search)

```

```{r}
#predict_svm=predict(svm_grid_search$best.model, data_test, type='raw')
```


```{r}
#matrice de confusion en utilisant caret
confusionMatrix(data=predict_svm,as.factor(data_test$class), positive = 'normal')
```


# RESEAUX DE NEURONNE
############################################################################################################################################################ RESEAUX DE NEURONNE #################################################################################################################################################################

```{r}
install.packages("neuralnet")
install.packages("keras")
install.packages("mltools")
```
```{r}
#library
require(neuralnet)
library(keras)
library(mltools)
library(data.table)
library(caret)


#Rename 
names(data_train)[19]<-"class"
names(data_test)[19]<-"class"
#one hot train 
dmy <- dummyVars("~ .",data=data_train)
data_train_onehot<- data.frame(predict(dmy, newdata = data_train))
#Drop colonne normal
data_train_onehot<- data_train_onehot[-20]

#one hot test 
dmy <- dummyVars("~ .",data=data_test)
data_test_onehot<- data.frame(predict(dmy, newdata = data_test))
#Drop colonne normal
data_train_onehot<- data_test_onehot[-20]

```


```{r}
normalize<-function(x){
  return((x-min(x))/max(x)-min(x))
}
maxmindf<-as.data.frame(lapply(data_train_onehot,normalize))
```

```{r}
#fit reseaux de neuronnes 
nn=neuralnet(classanomaly~.,data=maxmindf,hidden=1,act.fct = "logistic",linear.output = FALSE,threshold = 0.01)
plot(nn)
```

## Accuracy


```{r}
temp_test <- data_test_onehot[-19]
head(temp_test)
nn.results <- compute(nn,temp_test)
results <- data.frame(actual = data_test_onehot$classanomaly, prediction = nn.results$net.result)
print(results)
```


## Confusion matrix 

```{r}
roundedresults <- sapply(results, round , digits=0)
roundedresultsdf=data.frame(roundedresults)
attach(roundedresultsdf)
cm<-table(actual,prediction)
print(cm)
#Taux d'erreur
cm[2,1]/sum(cm)
```

#Elasticnet

```{r}
#Création des données
yTrain = as.matrix(data_train[,19])
XTrain = as.matrix(data_train[,-19])
yTest = as.matrix(data_test[,19])
XTest = as.matrix(data_test[,-19])

library(glmnet)
```

### test avec une serie de alpha 

```{r}
#creation une liste des hyperparametres
gs <- list(alpha = seq(from=0.1, to=0.9, by = 0.1)) 
#transformer gs en data frame grid
gs=expand.grid(gs)
gs
```

```{r}
#nombre de ligne de gs
n=nrow(gs)
#creation un vecteur de longueur n
err=rep(0,n)
#creation un vecteur de longueur n
rappel=rep(0,n)
#creation un vecteur de longueur n
precision=rep(0,n)
#Vecteur avec meilleur lamba
lambda = rep(0,n)

# recherche de grille
for (i in 1:n){
  cv.elnet = cv.glmnet(XTrain,yTrain,family="binomial",type.measure="class",nfolds=10,alpha=gs$alpha[i])
  #lambda min
  best_l = cv.elnet$lambda.min
  #prédiction
  pred.elnet = predict(cv.elnet,XTest,s=c(cv.elnet$lambda.min),type="class")
  conf=table(data_test$class,pred.elnet)
  e=1-sum(diag(conf))/sum(conf)
  rap=conf[2,2]/sum(conf["normal",])
  pre=conf[2,2]/sum(conf[,"normal"])
  err[i]=e
  rappel[i]=rap
  precision[i]=pre
  lambda[i] = best_l
}

#dataframe qui contient des erreurs, taux rappel et precision
gs=cbind(cbind(cbind(gs,lambda,err),rappel), precision)
print(gs)  
```

```{r}
#Ligne avec le plus petit taux d'erreur 
gs[gs$err==min(gs$err),]
```
 Le plus petit taux erreur (0.068) est avec un alpha 0.2 et lambda 0.0013. On peut le voir sur le graphique ci-dessous: 
 
```{r}
#Graphique avec en abscice le alpha et en ordonné les erreur 
plot(gs$err,gs$alpha)
```
Nous pouvons réaliser la validation croisé avec la valeur 0.2 pour verifier le lambda
```{r}
cv.elnet2 <-cv.glmnet(XTrain,yTrain,family="binomial",type.measure="class",nfolds=10,alpha=0.2)
#Graphique
plot(cv.elnet2)
#lambda min
print(cv.elnet2$lambda.min)
```


#Annexe 

## D'autre méthode

### TEST AVEC METHODE D"ENSEMBLE(PAS ENCORE CV) 

```{r}
install.packages("ipred")
library(ipred)
```

```{r}
data_ensemble=data_train[,]
data_ensemble$class=as.factor(data_train$class)
```



```{r}
set.seed(10)

data_train_ensemble <- bagging(class ~ ., data = data_ensemble, coob = TRUE, nbagg=55)

print(data_train_ensemble)
```


```{r}
predict_ensemble=predict(data_train_ensemble, data_test, type='class')
```



```{r}
# Matrice de confusion

conf3=table(data_test$class,predict_ensemble)
print(conf3)

# Taux d'erreur

print(1-sum(diag(conf3))/sum(conf3))

# Taux rappel

print(conf3[2,2]/sum(conf3["normal",]))

# Taux precision

print(conf3[2,2]/sum(conf3[,"normal"]))

```

```{r}
# Aire de ROC curve

#library(Metrics)
predict_ensemble_proba=predict(data_train_ensemble, data_test, type='prob')
auc(actual = ifelse(data_test$class== "normal", 1, 0), predicted = predict_ensemble_proba[,"normal"]) 

```

### TEST AVEC METHODE D"ENSEMBLE(AVEC CV)


```{r}
# Train avec CV
CV=trainControl(method = "cv", number = 5, classProbs = TRUE, summaryFunction = twoClassSummary)
data_ensemble_caret = train(class ~ ., data = data_ensemble, method = "treebag", metric = "ROC", trControl = CV)
print(data_ensemble_caret$results[,"ROC"])
```

```{r}
# Predire sur l'ensemble Test
predict_ensemble_proba_caret=predict(data_ensemble_caret, data_test, type='prob')
auc(actual = ifelse(data_test$class== "normal", 1, 0), predicted = predict_ensemble_proba_caret[,"normal"]) 
```



```{r}
predict_ensemble_caret=predict(data_ensemble_caret, data_test, type='raw')
```



```{r}
#matrice de confusion en utilisant caret
confusionMatrix(data=predict_ensemble_caret,as.factor(data_test$class), positive = 'normal')
```


### RANDOM FOREST


```{r}
library(randomForest)
```
```{r}
# preparer le data pour random forest

data_rf=data_train[,]
data_rf$class=as.factor(data_rf$class)
data_rf_x=subset(data_rf,select=-class)
data_rf_y=as.factor(data_rf$class) # il faut transformer en type factor

```



```{r}
# optimisez le hyperparametre mtry:

mtry_search = tuneRF(x=data_rf_x, y = data_rf_y, ntreeTry = 500)
               
mtry_opt <- mtry_search [,"mtry"][which.min(mtry_search [,"OOBError"])]
print(mtry_opt)

```



```{r}
# optimiser les hyperparametres mtry, nodesize, sampsize, ntree

# liste des valeurs des hyperparametres à tester
mtry = seq(4, 19, 2)
nodesize = seq(3, 8, 2)
sampsize = nrow(data_train) * c(0.5, 0.7, 0.8)
#ntree=seq(400,1000,100)

# le dataframe contient toutes les possibilites  
hyper_grid = expand.grid(mtry = mtry, nodesize = nodesize, sampsize = sampsize) # ntree=ntree)

# vecteur contient des erreurs
oob_err = c()

# recherche des hyperparametres:
for (i in 1:nrow(hyper_grid)) {
  model = randomForest(formula = class ~ ., data = data_rf, mtry = hyper_grid$mtry[i],
                          nodesize = hyper_grid$nodesize[i],
                          sampsize = hyper_grid$sampsize[i])
                          
                       
  oob_err[i] = model$err.rate[nrow(model$err.rate), "OOB"]
}

# identifier les meilleurs hyperparametres
opt_i = which.min(oob_err)
print(hyper_grid[opt_i,])

```



```{r}
# train et test avec le meilleur model

model = randomForest(formula = class ~ ., data = data_rf, mtry = 6, nodesize = 3, sampsize = 13225)
predict_rf=predict(model,newdata = data_test, type = 'class')

```



```{r}
confusionMatrix(data=as.factor(predict_rf),as.factor(data_test$class), positive = 'normal')
```



### Gradient Boosting

```{r}
library(gbm)
```


```{r}
data_gb=data_train[,]
data_gb$class=ifelse(data_gb$class=='normal',1,0)
```


```{r}
gb_model=gbm(formula = class~.,distribution = 'bernoulli', cv.folds = 15, data = data_gb, n.trees = 10000)
```



```{r}
# optimiser le nombre d'arbre

ntree_gb <- gbm.perf(gb_model, method = 'cv')
print(ntree_gb)
```

```{r}
data_gb_test=data_test[,]
data_gb_test$class=ifelse(data_gb_test$class=='normal',1,0)
```


```{r}
predict_gb=predict(gb_model,newdata = data_gb_test, n.trees = 9753, type = 'response')
```

```{r}
#matrice de confusion en utilisant caret
library(caret)
prediction_gb=ifelse(predict_gb<0.5,0,1)
confusionMatrix(data=as.factor(prediction_gb),as.factor(data_gb_test$class), positive = '0')
```


```{r}
library(Metrics)
auc(actual = data_gb_test$class, predict_gb)
```
## ROC Curve


```{r}
# liste des predictions avec bagged trees et gradient boosting
preds_list <- list(predict_ensemble_proba[,2], predict_gb)

# liste des valeurs reels
m <- length(preds_list)
actuals_list <- rep(list(data_gb_test$class), m)

# Plot the ROC curves
pred <- prediction(preds_list, actuals_list)
rocs <- performance(pred, "tpr", "fpr")
plot(rocs, col = as.list(1:m), main = "Courbe de ROC sur l'ensemble test")
legend(x = "bottomright", 
       legend = c("Bagged Trees", "GBM"),
       fill = 1:m)

```






